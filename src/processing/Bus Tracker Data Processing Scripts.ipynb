{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import arrow\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [To Do](#todo)\n",
    "* [Loading Raw Vehicle Position Data](#loading)\n",
    "* [Downloading and Parsing Patterns](#patterns)\n",
    "* [Cleaning the raw data](#cleaning)\n",
    "* [Building timetables](#timetable)\n",
    "* [Derived Data](#derived)\n",
    "    * [Trip and Wait Times](#trips_waits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "This notebook contains the scripts used to clean and transform bus position data collected from the **`getvehicles`** API. The data processing is conducted as follows:\n",
    "1. Load the raw vehicle position data into a pandas DataFrame. Filter out any undesired bus routes.\n",
    "2. Download bus route patterns from the **`getpatterns`** API.\n",
    "3. Load route patterns into a pandas DataFrame.\n",
    "4. Clean the raw vehicle position data, including removing duplicate rows, and assigning each bus trip a unique ID.\n",
    "5. For each bus route, transform the raw vehicle data into a timetable DataFrame, with bus stops as the columns and individual trips as rows.\n",
    "6. Using each bus route's timetable DataFrame, calculate the travel times between each bus stop and the wait times between adjacent buses at each stop, and store in a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do <a name=\"todo\"></a>\n",
    "* Breakup Jupyter notebook into standalone Python scripts for data processing\n",
    "* Flag bad vehicle position data? e.g. Check monotonicity, count number of data points.\n",
    "* Write functions to keep track of access date when downloading patterns from the **`getpatterns`** API, and create new file if pattern has changed\n",
    "* Load raw pattern data in a more elegant and simpler way. Current loading process is a holdover from a different way of processing patterns data.\n",
    "* Edit code for consistent style.\n",
    "* Completed detailed function descriptions and useage notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Raw Vehicle Position Data <a name=\"loading\"></a>\n",
    "\n",
    "`load_raw_data`  \n",
    "Loads raw vehicle position data from possibly multiple file sources into one DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_raw_data(path, file_stem):\n",
    "    names = ['tripid', 'tmstmp', 'pid', 'rt', 'pdist', 'dly']\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(path, \"*{}.csv\".format(file_stem)))\n",
    "    df_each = (pd.read_csv(f, skiprows=1, names=names, dtype=str) for f in all_files)\n",
    "    df = pd.concat(df_each, ignore_index=True)\n",
    "    \n",
    "    df.dropna(how='any', inplace=True)\n",
    "    df.tmstmp =  pd.to_datetime(df.tmstmp)\n",
    "    df.pdist = df.pdist.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Parsing Patterns <a name=\"patterns\"></a>\n",
    "\n",
    "`check_if_path_exists`  \n",
    "Checks if the given directories exists. If they do not exist, it creates them.\n",
    "\n",
    "`get_patterns`  \n",
    "Queries the **`getpatterns`** API to obtain information patterns present in given DataFrame. Each pattern is written to a separate .json file of the form *RouteNumber_PatternID.json*.\n",
    "\n",
    "Note: It is not possible to use the **`getpatterns`** API to access all patterns associated with a particular bus route. One either has the option to request all *active* patterns for a particular bus route or to request patterns individually by their pattern ID. This script does the latter.\n",
    "\n",
    "`untangle_pattern_columns` and `pattern_dict_to_df` are supporting functions for `load_patterns`.\n",
    "\n",
    "`load_patterns`\n",
    "Loads the patterns for a particular bus route into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.read(\"../../keys.config\")\n",
    "API_KEY = config.get(\"ctabustracker\", \"api_key\")\n",
    "URL = \"http://www.ctabustracker.com/bustime/api/v2/getpatterns\"\n",
    "patterns_path = \"../../data/raw/getpatterns/\"\n",
    "\n",
    "def check_if_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "\n",
    "def get_patterns(df):\n",
    "    check_if_path_exists(patterns_path)\n",
    "    \n",
    "    pids = [str(pid) for pid in df.pid.unique()]\n",
    "    # the getpatterns API only accepts upto 10 patterns at a time\n",
    "    pids_chunks = [pids[i:i+10] for i in xrange(0, len(pids), 10)]\n",
    "    for chunk in pids_chunks:\n",
    "        pids_str = \",\".join(chunk)\n",
    "        payload = {'key': API_KEY, 'pid': pids_str, 'format': \"json\"}      \n",
    "        r = requests.get(URL, params=payload)\n",
    "        patterns = r.json().get('bustime-response').get('ptr')\n",
    "        \n",
    "        if not patterns:\n",
    "            print \"A very bad error has occurred\"\n",
    "              \n",
    "        for pattern in patterns:\n",
    "            pid = str(pattern['pid'])\n",
    "            rt = str(df[df.pid == pid].rt.unique()[0])\n",
    "            with open(os.path.join(patterns_path, \"{}_{}.json\".format(rt, pid)), 'w') as out_file:\n",
    "                json.dump(pattern, out_file)\n",
    "                \n",
    "def untangle_pattern_columns(df, pid):\n",
    "    stops_df = pd.DataFrame(df.loc['pt', pid])\n",
    "    pattern_info_df = pd.DataFrame(df.loc[~df.index.isin(['pt']), pid]).T.reset_index(drop=True)\n",
    "    pattern_df = stops_df.join(pattern_info_df)\n",
    "    ff_cols = ['first', 'last', 'ln', 'pid', 'rtdir']\n",
    "    pattern_df[ff_cols] = pattern_df[ff_cols].ffill()\n",
    "    pattern_df.pid = pattern_df.pid.astype(str)\n",
    "    return pattern_df\n",
    "\n",
    "def pattern_dict_to_df(json_dict):\n",
    "    df = pd.DataFrame(json_dict)\n",
    "    new_df = pd.concat([untangle_pattern_columns(df, pid) for pid in df.columns]).reset_index(drop=True)\n",
    "    new_df.drop(new_df[new_df.typ == \"W\"].index, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def load_patterns(rt):\n",
    "    pattern_dict = {}\n",
    "    \n",
    "    for file in glob.glob(os.path.join(patterns_path, \"{}_*\".format(rt, pid))):\n",
    "        with open(file) as f:    \n",
    "            pattern = json.load(f)\n",
    "            \n",
    "        stops = [stop for stop in pattern['pt'] if stop['typ'] != \"W\"]\n",
    "        pattern['first'] = stops[0]['stpnm']\n",
    "        pattern['last'] = stops[-1]['stpnm']\n",
    "        pattern_dict[pattern['pid']] = pattern\n",
    "    \n",
    "    return pattern_dict_to_df(pattern_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the raw data <a name=\"cleaning\"></a>\n",
    "\n",
    "`remove_unknown_patterns`  \n",
    "Removes trips with pIDs for which there is no pattern data.\n",
    "\n",
    "`create_unique_id`  \n",
    "Tripids alone are not necessarily unique identifiers of a trip on a given route. This function creates a unique identifier for each trip by combining the start date of the trip, the pID of pattern it is executing, and its CTA-assigned tripid.\n",
    "\n",
    "Note: For the purposes of the CTA's scheduling, the new service day starts around 3AM. For example, if trip initially departs at 2AM on 2017-01-02, then the trip was scheduled for the 2017-01-01 service day. **Similarly, in this project 3AM US/Central time is treated as the start of a new day.**\n",
    "\n",
    "`remove_short_trips`  \n",
    "Removes data where bus travels less than 5,000 ft.\n",
    "\n",
    "`clean`  \n",
    "Removes duplicate rows and executes the above three functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_unknown_patterns(df, patterns):    \n",
    "    not_include = (set(df.pid) - set(patterns.pid))\n",
    "    df.drop(df[df.pid.isin(not_include)].index, inplace=True)\n",
    "    print \"Deleted pattern IDs {}\".format(list(not_include))\n",
    "\n",
    "def create_unique_ids(df):\n",
    "    df[\"unix_tmstmp\"] = df.tmstmp.apply(lambda x: arrow.get(x, 'US/Central').timestamp)\n",
    "    df.sort_values(['tripid', 'tmstmp'], inplace=True)\n",
    "    # If two data points with same tripID are more than 30 minutes a part, they probably belong to different trips\n",
    "    # In practice, such data points will usually (but not always), be at least 24 hours apart.\n",
    "    g = df.groupby(['tripid', (df.tmstmp.diff() > pd.Timedelta('30 minutes')).astype(int).cumsum()])\n",
    "    idxmins = g.unix_tmstmp.idxmin()\n",
    "    df_idxmins = df.loc[idxmins]\n",
    "    df.loc[idxmins, \"ID\"] = (df_idxmins.tmstmp - pd.DateOffset(hours=3)).dt.strftime('%Y%m%d') + \"_\" + df_idxmins.pid + \"_\" + df_idxmins.tripid\n",
    "    df.ID.ffill(inplace=True)\n",
    "\n",
    "def remove_short_trips(df):\n",
    "    df.drop(df.groupby('ID').filter(lambda x: x.pdist.max() - x.pdist.min() < 5000).index, inplace=True)\n",
    "            \n",
    "def clean(df, patterns):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    remove_unknown_patterns(df, patterns)\n",
    "    create_unique_ids(df)\n",
    "    remove_short_trips(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building timetables <a name=\"timetable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interpolate_arrival_times`  \n",
    "Interpolates the DateTimes that the given bus trip reaches all of the provided stops along its route. Input should be given as a DataFrame with rows corresponding only to a single trip. Returns a list of date strings or np.nan if the bus does not reach a particular stop. Interpolation is performed as follows: given the raw location data for a single trip and the location of a bus stop, first sort the data into two parts: all of the raw data points where the bus is before the bus stop and all of the raw data points where the bus is at or past the bus stop. Next, select the row from the before table with the latest timestamp (before_max) and select the row from the after table with the earliest timestamp (after_min). Find the equation of the line connecting these two points where the input is pdist (distance) and the output is timestamp (time) and evaluate the equation at the pdist of the bus stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_query_strings(stop, patterns):\n",
    "    filtered = patterns[patterns.stpnm == stop][[\"pid\", \"pdist\"]]\n",
    "    #stop_pdist = shift_terminal_stop_pdist(patterns, pattern, stop)\n",
    "\n",
    "    query_str_before = \" | \".join([\"(pid == '{}' & pdist < {})\".format(pid, pdist) for pid, pdist in zip(filtered.pid, filtered.pdist)])\n",
    "    query_str_after = \" | \".join([\"(pid == '{}' & pdist >= {})\".format(pid, pdist) for pid, pdist in zip(filtered.pid, filtered.pdist)])\n",
    "    return query_str_before, query_str_after\n",
    "\n",
    "def find_linear_interpolant_endpoints(df, stop, patterns):\n",
    "    query_str_before, query_str_after = build_query_strings(stop, patterns)\n",
    "\n",
    "    idxmaxs = df.query(query_str_before).groupby('ID').unix_tmstmp.idxmax()\n",
    "    idxmins = df.query(query_str_after).groupby('ID').unix_tmstmp.idxmin()\n",
    "\n",
    "    before = df.loc[idxmaxs, [\"pdist\", \"unix_tmstmp\", \"ID\"]].set_index(\"ID\")\n",
    "    after = df.loc[idxmins, [\"pdist\", \"unix_tmstmp\", \"ID\"]].set_index(\"ID\")\n",
    "\n",
    "    return before, after\n",
    "\n",
    "def evaluate_linear_interpolant(tableaux, stop):\n",
    "    interpolated_arrivals = (\n",
    "            ((tableaux.unix_tmstmp_after - tableaux.unix_tmstmp) / (tableaux.pdist_after - tableaux.pdist))\n",
    "            * (tableaux.stop_pdist - tableaux.pdist)\n",
    "            + tableaux.unix_tmstmp\n",
    "        ) \n",
    "    mask = pd.notnull(interpolated_arrivals)\n",
    "    interpolated_arrivals.loc[mask] = interpolated_arrivals[mask].map(lambda x: arrow.get(x).to('US/Central').format('YYYY-MM-DD HH:mm:ss'))\n",
    "    return interpolated_arrivals\n",
    "\n",
    "def interpolate_stop_arrival_times(df, stop, patterns):\n",
    "    tableaux = pd.DataFrame(np.nan, index=df.ID.unique(), columns=[stop])\n",
    "    tableaux = tableaux.join(df.groupby('ID').pid.first())\n",
    "\n",
    "    before, after = find_linear_interpolant_endpoints(df, stop, patterns)\n",
    "    tableaux = tableaux.join(before, rsuffix=\"_before\").join(after, rsuffix=\"_after\")\n",
    "    \n",
    "    pid_to_pdist = patterns[patterns.stpnm == stop].groupby('pid').pdist.first()\n",
    "    mask = tableaux.pid.isin(pid_to_pdist.index)\n",
    "    tableaux.loc[mask, \"stop_pdist\"] = tableaux[mask].pid.map(pid_to_pdist)\n",
    "\n",
    "    interpolated_arrivals = evaluate_linear_interpolant(tableaux, stop)\n",
    "    return interpolated_arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_bidirectional_timetable`  \n",
    "Builds a timetable DataFrame of interpolated arrival/departure times for each bus stop in either service direction on given route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Holiday schedules\n",
    "# Our services operate on a Sunday schedule on New Year’s Day, Memorial Day,\n",
    "# July 4th (Independence Day), Labor Day, Thanksgiving Day and Christmas Day.\n",
    "holidays = [\n",
    "    \"2017-01-01\", \"2017-05-29\", \"2017-07-04\", \"2018-09-04\", \"2018-11-23\", \"2017-12-25\",\n",
    "    \"2018-01-01\", \"2018-05-28\", \"2018-07-04\", \"2018-09-03\", \"2018-11-22\", \"2018-12-25\",\n",
    "    \"2019-01-01\", \"2019-05-27\", \"2019-07-04\", \"2019-09-02\", \"2019-11-28\", \"2019-12-25\"\n",
    "]\n",
    "cta_holidays = pd.DatetimeIndex(holidays)\n",
    "timetables_path = \"../../data/processed/timetables/\"\n",
    "\n",
    "def build_bidirectional_timetable(df, patterns):\n",
    "    stop_list = patterns.stpnm.dropna().unique()\n",
    "    timetable = pd.DataFrame(np.nan, index=df.ID.unique(), columns=stop_list)\n",
    "    timetable.index.name = \"ID\"\n",
    "\n",
    "    for stop in stop_list:\n",
    "        timetable[stop] = interpolate_stop_arrival_times(df, stop, patterns)\n",
    "    \n",
    "    timetable.reset_index(inplace=True)\n",
    "    timetable[[\"start_date\", \"pid\", \"tatripid\"]] = timetable.ID.str.split(\"_\", expand=True)\n",
    "    timetable[\"rtdir\"] = timetable.pid.map(patterns.groupby('pid').rtdir.first())\n",
    "    timetable[\"start_date\"] = pd.to_datetime(timetable.start_date)\n",
    "    timetable[\"day_of_week\"] = timetable.start_date.dt.dayofweek\n",
    "    timetable[\"holiday\"] = timetable.start_date.isin(cta_holidays)\n",
    "    return timetable\n",
    "\n",
    "def write_timetables(df):\n",
    "    timetable = interpolate_arrival_times(df)\n",
    "    check_if_path_exists(timetables_path)\n",
    "    #split timetable into two pieces based on patterns\n",
    "    #.to_csv(timetables_path + str(rt) + \"_timetable_\" + (direction[0].lower() + \"b\") + \".csv\", index=False)\n",
    "    \n",
    "def load_timetable(filename):\n",
    "    timetable = pd.read_csv(timetables_path + filename)\n",
    "    timetable[stop_list] = timetable[stop_list].apply(pd.to_datetime)\n",
    "    return timetable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shift_terminal_stop_pdists`  \n",
    "There is a significant amount of noise at the terminal stops of a route. This function shifts the location of the terminal stops inward by 500 feet. For example, if the first stop is located has a pdist of 0ft and the final stop has a pdist of 50,000ft, then `interpolate_arrival_times` treats these stops as being located at 500ft and 49,500ft, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_terminal_stop_pdists(patterns):\n",
    "    patterns.loc[np.intersect1d(patterns.groupby('pid').seq.idxmin(), patterns[patterns.pdist < 500].index), \"pdist\"] = 500\n",
    "\n",
    "    idxs = np.intersect1d(patterns.groupby('pid').seq.idxmax(), patterns[patterns.pdist > patterns.ln - 500].index)\n",
    "    pattern_lengths = patterns.loc[idxs].ln \n",
    "    patterns.loc[idxs, \"pdist\"] = pattern_lengths - 500\n",
    "    \n",
    "def process_raw_data(path, stem, rt):\n",
    "    df = load_raw_data(path, stem)\n",
    "    df.drop(df[df.rt != rt].index, inplace=True)\n",
    "    patterns = load_patterns(rt)\n",
    "    shift_terminal_stop_pdists(patterns)\n",
    "    clean(df, patterns)\n",
    "    build_timetable(df, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived Data<a name=\"#derived\"></a>\n",
    "### Trip and Wait Times<a name=\"#trips_waits\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_destination_stops(patterns, stop, direction):\n",
    "    filtered = patterns[(patterns.stpnm == stop) & (patterns.rtdir == direction)]\n",
    "    query_str = \" | \".join([\"(pid == '{}' & seq > {})\".format(pid, seq) for pid, seq in zip(filtered.pid, filtered.seq)])\n",
    "    return list(patterns.query(query_str).stpnm.unique())\n",
    "\n",
    "def timedelta_to_decimal(td):\n",
    "    return round(abs((td / np.timedelta64(1, 'D')) * 1440), 2)\n",
    "\n",
    "def calculate_travel_times(df, origin, destinations):\n",
    "    df[destinations] = df[destinations].sub(df[origin], axis=0)\n",
    "    df[destinations] = df[destinations].apply(timedelta_to_decimal)\n",
    "    return df\n",
    "\n",
    "def calculate_wait_times(df, stop):\n",
    "    df[\"wait_time\"] = df[stop] - df[stop].shift(-1)\n",
    "        \n",
    "    # removes wait times calculated between service days\n",
    "    df[\"day_diff\"] = df.start_date - df.start_date.shift(-1)\n",
    "    df.loc[df.day_diff.dt.days != 0, \"wait_time\"] = None\n",
    "    \n",
    "    df.drop(columns=[stop, \"day_diff\"], inplace=True)\n",
    "    df.wait_time = df.wait_time.apply(timedelta_to_decimal)\n",
    "    return df\n",
    "\n",
    "def build_travel_waits_df(df, patterns, direction):\n",
    "    info_columns = [\"start_date\", \"pid\", \"tatripid\", \"rtdir\", \"day_of_week\", \"holiday\", \"decimal_time\", \"wait_time\"]\n",
    "    \n",
    "    stop_list = patterns[patterns.rtdir == direction].stpnm.dropna().unique()\n",
    "    directional_df = df.loc[df.rtdir == direction]\n",
    "    directional_df[stop_list] = directional_df[stop_list].apply(pd.to_datetime)\n",
    "    \n",
    "    travels_waits = []\n",
    "    for origin in stop_list:\n",
    "        destinations = get_destination_stops(patterns, origin, direction)\n",
    "        \n",
    "        if not destinations:\n",
    "            continue\n",
    "\n",
    "        orgin_and_dests = [origin] + destinations\n",
    "        sorted_df = directional_df.sort_values(by=origin) # sorting by arrival times in origin column\n",
    "        sorted_df[\"decimal_time\"] = sorted_df[origin].map(lambda x: round(x.hour + x.minute / 60.0, 2))\n",
    "\n",
    "        sorted_df = calculate_travel_times(sorted_df, origin, destinations)        \n",
    "        sorted_df = calculate_wait_times(sorted_df, origin)\n",
    "\n",
    "        melted_df = pd.melt(sorted_df, id_vars=info_columns, value_vars=destinations, var_name=\"destination\", value_name=\"travel_time\")\n",
    "        melted_df.dropna(subset=[\"wait_time\", \"travel_time\"], inplace=True)\n",
    "        melted_df[\"origin\"] = origin\n",
    "        travels_waits.append(melted_df)\n",
    "    return pd.concat(travels_waits, ignore_index=True)\n",
    "\n",
    "def write_travel_waits():\n",
    "    #travel_waits_path = \"../../data/processed/trips_and_waits/\" + str(rt) + \"/\"\n",
    "    #check_if_path_exists(travel_waits_path)\n",
    "    #file_name = travel_waits_path + origin.replace(\"/\", \"\").replace(\".\", \"\") + \"_\" + direction + \".csv\"\n",
    "    #melted_df.to_csv(file_name, columns=header, header=False, index=False, mode='ab+')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
