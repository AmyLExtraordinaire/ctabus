{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import arrow\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [To Do](#todo)\n",
    "* [Loading Raw Vehicle Position Data](#loading)\n",
    "* [Downloading and Parsing Patterns](#patterns)\n",
    "* [Cleaning the raw data](#cleaning)\n",
    "* [Building timetables](#timetable)\n",
    "* [Derived Data](#derived)\n",
    "    * [Trip and Wait Times](#trips_waits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "This notebook contains the scripts used to clean and transform bus position data collected from the **`getvehicles`** API. The data processing is conducted as follows:\n",
    "1. Load the raw vehicle position data into a pandas DataFrame. Filter out any undesired bus routes.\n",
    "2. Download bus route patterns from the **`getpatterns`** API.\n",
    "3. Load route patterns into a pandas DataFrame.\n",
    "4. Clean the raw vehicle position data, including removing duplicate rows, and assigning each bus trip a unique ID.\n",
    "5. For each bus route, transform the raw vehicle data into a timetable DataFrame, with bus stops as the columns and individual trips as rows.\n",
    "6. Using each bus route's timetable DataFrame, calculate the travel times between each bus stop and the wait times between adjacent buses at each stop, and store in a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do <a name=\"todo\"></a>\n",
    "* Breakup Jupyter notebook into standalone Python scripts for data processing\n",
    "* Flag bad vehicle position data? e.g. Check monotonicity, count number of data points.\n",
    "* Write functions to keep track of access date when downloading patterns from the **`getpatterns`** API, and create new file if pattern has changed\n",
    "* Edit code for consistent style.\n",
    "* Completed detailed function descriptions and useage notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Raw Vehicle Position Data <a name=\"loading\"></a>\n",
    "\n",
    "`load_raw_data`  \n",
    "Loads raw vehicle position data from possibly multiple file sources into one DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_raw_data(path, file_stem):\n",
    "    names = ['tripid', 'tmstmp', 'pid', 'rt', 'pdist', 'dly']\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(path, \"*{}.csv\".format(file_stem)))\n",
    "    df_each = (pd.read_csv(f, skiprows=1, names=names, dtype=str) for f in all_files)\n",
    "    df = pd.concat(df_each, ignore_index=True)\n",
    "    \n",
    "    df.dropna(how='any', inplace=True)\n",
    "    df.tmstmp =  pd.to_datetime(df.tmstmp)\n",
    "    df.pdist = df.pdist.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Parsing Patterns <a name=\"patterns\"></a>\n",
    "\n",
    "`check_if_path_exists`  \n",
    "Checks if the given directories exists. If they do not exist, it creates them.\n",
    "\n",
    "`get_patterns`  \n",
    "Calls the **`getpatterns`** API to obtain information patterns present in given DataFrame. Each pattern is written to a separate .json file of the form *RouteNumber_PatternID.json*.\n",
    "\n",
    "Note: It is not possible to use the **`getpatterns`** API to access all patterns associated with a particular bus route. One either has the option to request all *active* patterns for a particular bus route or to request patterns individually by their pattern ID. This script does the latter.\n",
    "\n",
    "`load_patterns`\n",
    "Loads the patterns for a particular bus route into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.read(\"../../keys.config\")\n",
    "API_KEY = config.get(\"ctabustracker\", \"api_key\")\n",
    "URL = \"http://www.ctabustracker.com/bustime/api/v2/getpatterns\"\n",
    "patterns_path = \"../../data/raw/getpatterns/\"\n",
    "\n",
    "def check_if_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "\n",
    "def get_patterns(df):\n",
    "    check_if_path_exists(patterns_path)\n",
    "    \n",
    "    pids = [str(pid) for pid in df.pid.unique()]\n",
    "    # the getpatterns API only accepts upto 10 patterns at a time\n",
    "    pids_chunks = [pids[i:i+10] for i in xrange(0, len(pids), 10)]\n",
    "    for chunk in pids_chunks:\n",
    "        pids_str = \",\".join(chunk)\n",
    "        payload = {'key': API_KEY, 'pid': pids_str, 'format': \"json\"}      \n",
    "        r = requests.get(URL, params=payload)\n",
    "        patterns = r.json().get('bustime-response').get('ptr')\n",
    "        \n",
    "        if not patterns:\n",
    "            print \"A very bad error has occurred\"\n",
    "              \n",
    "        for pattern in patterns:\n",
    "            pid = str(pattern['pid'])\n",
    "            rt = str(df[df.pid == pid].rt.unique()[0])\n",
    "            with open(os.path.join(patterns_path, \"{}_{}.json\".format(rt, pid)), 'w') as out_file:\n",
    "                json.dump(pattern, out_file)\n",
    "                \n",
    "def load_patterns(rt):\n",
    "    dfs = []\n",
    "    for file in glob.glob(os.path.join(patterns_path, \"{}_*\".format(rt))):\n",
    "        with open(file) as f:\n",
    "            pattern_json = json.load(f)\n",
    "        stops = [stop for stop in pattern_json.get('pt') if stop.get('typ') != \"W\"]\n",
    "        df = pd.DataFrame(stops)\n",
    "        pattern_json.pop('pt')\n",
    "        dfs.append(df.assign(**pattern_json))\n",
    "        patterns = pd.concat(dfs, ignore_index=True)\n",
    "        patterns.pid = patterns.pid.astype(str)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the raw data <a name=\"cleaning\"></a>\n",
    "\n",
    "`remove_unknown_patterns`  \n",
    "Removes trips with pIDs for which there is no pattern data.\n",
    "\n",
    "`create_unique_id`  \n",
    "Tripids alone are not necessarily unique identifiers of a trip on a given route. This function creates a unique identifier for each trip by combining the start date of the trip, the pID of pattern it is executing, and its CTA-assigned tripid.\n",
    "\n",
    "Note: For the purposes of the CTA's scheduling, the new service day starts around 3AM. For example, if trip initially departs at 2AM on 2017-01-02, then the trip was scheduled for the 2017-01-01 service day. **Similarly, in this project 3AM US/Central time is treated as the start of a new day.**\n",
    "\n",
    "`remove_short_trips`  \n",
    "Removes data where bus travels less than 5,000 ft.\n",
    "\n",
    "`clean`  \n",
    "Removes duplicate rows and executes the above three functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_unknown_patterns(df, patterns):    \n",
    "    not_include = (set(df.pid) - set(patterns.pid))\n",
    "    df.drop(df[df.pid.isin(not_include)].index, inplace=True)\n",
    "    print \"Deleted pattern IDs {}\".format(list(not_include))\n",
    "\n",
    "def create_unique_ids(df):\n",
    "    df[\"unix_tmstmp\"] = df.tmstmp.apply(lambda x: arrow.get(x, 'US/Central').timestamp)\n",
    "    df.sort_values(['tripid', 'tmstmp'], inplace=True)\n",
    "    # If two data points with same tripID are more than 30 minutes a part, they probably belong to different trips\n",
    "    # In practice, such data points will usually (but not always), be at least 24 hours apart.\n",
    "    g = df.groupby(['tripid', (df.tmstmp.diff() > pd.Timedelta('30 minutes')).astype(int).cumsum()])\n",
    "    idxmins = g.unix_tmstmp.idxmin()\n",
    "    df_idxmins = df.loc[idxmins]\n",
    "    df.loc[idxmins, \"ID\"] = (df_idxmins.tmstmp - pd.DateOffset(hours=3)).dt.strftime('%Y%m%d') + \"_\" + df_idxmins.pid + \"_\" + df_idxmins.tripid\n",
    "    df.ID.ffill(inplace=True)\n",
    "\n",
    "def remove_short_trips(df):\n",
    "    df.drop(df.groupby('ID').filter(lambda x: x.pdist.max() - x.pdist.min() < 5000).index, inplace=True)\n",
    "            \n",
    "def clean(df, patterns):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    remove_unknown_patterns(df, patterns)\n",
    "    create_unique_ids(df)\n",
    "    remove_short_trips(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building timetables <a name=\"timetable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_query_strings`  \n",
    "Builds string to query DataFrame of raw data to determine the raw data point immediately before the buses arrived at the given stop and the raw data point immediately after the bus departed from that stop.\n",
    "\n",
    "`find_linear_interpolant_endpoints`  \n",
    "Finds the raw data points immediately before the buses arrived at the given stop and the raw data point immediately after the bus departed from that stop.\n",
    "\n",
    "`build_interpolation_table`  \n",
    "Organizes raw vehicle position data into tabular form to speed up interpolation calculation. Each row corresponds to a unique bus trip, and the columns correspond to values to be substitued into the formula for linear interpolation.\n",
    "\n",
    "`interpolate_stop_arrival_times`  \n",
    "Given a DataFrame of raw vehicle position data, a DataFrame of the bus route's patterns, and a bus stop, performs [linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) to determine the time at which the observed buses arrived at the given bus stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_query_strings(stop, patterns):\n",
    "    filtered = patterns[patterns.stpnm == stop][[\"pid\", \"pdist\"]]\n",
    "    #stop_pdist = shift_terminal_stop_pdist(patterns, pattern, stop)\n",
    "\n",
    "    query_str_before = \" | \".join([\"(pid == '{}' & pdist < {})\".format(pid, pdist) for pid, pdist in zip(filtered.pid, filtered.pdist)])\n",
    "    query_str_after = \" | \".join([\"(pid == '{}' & pdist >= {})\".format(pid, pdist) for pid, pdist in zip(filtered.pid, filtered.pdist)])\n",
    "    return query_str_before, query_str_after\n",
    "\n",
    "def find_linear_interpolant_endpoints(df, stop, patterns):\n",
    "    query_str_before, query_str_after = build_query_strings(stop, patterns)\n",
    "\n",
    "    idxmaxs = df.query(query_str_before).groupby('ID').unix_tmstmp.idxmax()\n",
    "    idxmins = df.query(query_str_after).groupby('ID').unix_tmstmp.idxmin()\n",
    "\n",
    "    before = df.loc[idxmaxs, [\"pdist\", \"unix_tmstmp\", \"ID\"]].set_index(\"ID\")\n",
    "    after = df.loc[idxmins, [\"pdist\", \"unix_tmstmp\", \"ID\"]].set_index(\"ID\")\n",
    "\n",
    "    return before, after\n",
    "\n",
    "def build_interpolation_table(df, stop, patterns):\n",
    "    table = pd.DataFrame(np.nan, index=df.ID.unique(), columns=[stop])\n",
    "    table = table.join(df.groupby('ID').pid.first())\n",
    "\n",
    "    before, after = find_linear_interpolant_endpoints(df, stop, patterns)\n",
    "    table = table.join(before, rsuffix=\"_before\").join(after, rsuffix=\"_after\")\n",
    "\n",
    "    pid_to_pdist = patterns[patterns.stpnm == stop].groupby('pid').pdist.first()\n",
    "    mask = table.pid.isin(pid_to_pdist.index)\n",
    "    table.loc[mask, \"stop_pdist\"] = table[mask].pid.map(pid_to_pdist)\n",
    "\n",
    "    return table\n",
    "\n",
    "def interpolate_stop_arrival_times(df, stop, patterns):\n",
    "    table = build_interpolation_table(df, stop, patterns)\n",
    "    interpolated_arrivals = (\n",
    "        ((table.unix_tmstmp_after - table.unix_tmstmp) / (table.pdist_after - table.pdist))\n",
    "        * (table.stop_pdist - table.pdist)\n",
    "        + table.unix_tmstmp\n",
    "    ) \n",
    "    mask = pd.notnull(interpolated_arrivals)\n",
    "    interpolated_arrivals.loc[mask] = interpolated_arrivals[mask].map(lambda x: arrow.get(x).to('US/Central').format('YYYY-MM-DD HH:mm:ss'))\n",
    "    return interpolated_arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_bidirectional_timetable`  \n",
    "Builds a timetable DataFrame of interpolated arrival/departure times for each bus stop in either service direction on given route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Holiday schedules\n",
    "# Our services operate on a Sunday schedule on New Year’s Day, Memorial Day,\n",
    "# July 4th (Independence Day), Labor Day, Thanksgiving Day and Christmas Day.\n",
    "holidays = [\n",
    "    \"2017-01-01\", \"2017-05-29\", \"2017-07-04\", \"2018-09-04\", \"2018-11-23\", \"2017-12-25\",\n",
    "    \"2018-01-01\", \"2018-05-28\", \"2018-07-04\", \"2018-09-03\", \"2018-11-22\", \"2018-12-25\",\n",
    "    \"2019-01-01\", \"2019-05-27\", \"2019-07-04\", \"2019-09-02\", \"2019-11-28\", \"2019-12-25\"\n",
    "]\n",
    "cta_holidays = pd.DatetimeIndex(holidays)\n",
    "timetables_path = \"../../data/processed/timetables/\"\n",
    "\n",
    "def build_bidirectional_timetable(df, patterns):\n",
    "    stop_list = patterns.stpnm.dropna().unique()\n",
    "    timetable = pd.DataFrame(np.nan, index=df.ID.unique(), columns=stop_list)\n",
    "    timetable.index.name = \"ID\"\n",
    "\n",
    "    for stop in stop_list:\n",
    "        timetable[stop] = interpolate_stop_arrival_times(df, stop, patterns)\n",
    "    \n",
    "    timetable.reset_index(inplace=True)\n",
    "    timetable[[\"start_date\", \"pid\", \"tatripid\"]] = timetable.ID.str.split(\"_\", expand=True)\n",
    "    timetable[\"rtdir\"] = timetable.pid.map(patterns.groupby('pid').rtdir.first())\n",
    "    timetable[\"start_date\"] = pd.to_datetime(timetable.start_date)\n",
    "    timetable[\"day_of_week\"] = timetable.start_date.dt.dayofweek\n",
    "    timetable[\"holiday\"] = timetable.start_date.isin(cta_holidays)\n",
    "    return timetable\n",
    "\n",
    "def write_timetable(timetable, rt):\n",
    "    check_if_path_exists(timetables_path)\n",
    "    out_path = os.path.join(timetables_path, \"{}_timetable.csv\".format(rt))\n",
    "    timetable.to_csv(out_path, index=False)\n",
    "\n",
    "def load_timetable(filename):\n",
    "    timetable = pd.read_csv(timetables_path + filename)\n",
    "    timetable[stop_list] = timetable[stop_list].apply(pd.to_datetime)\n",
    "    return timetable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shift_terminal_stop_pdists`  \n",
    "There is a significant amount of noise at the terminal stops of a route. This function shifts the location of the terminal stops inward by 500 feet. For example, if the first stop is located has a pdist of 0ft and the final stop has a pdist of 50,000ft, then `interpolate_arrival_times` treats these stops as being located at 500ft and 49,500ft, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_terminal_stop_pdists(patterns):\n",
    "    patterns.loc[np.intersect1d(patterns.groupby('pid').seq.idxmin(), patterns[patterns.pdist < 500].index), \"pdist\"] = 500\n",
    "\n",
    "    idxs = np.intersect1d(patterns.groupby('pid').seq.idxmax(), patterns[patterns.pdist > patterns.ln - 500].index)\n",
    "    pattern_lengths = patterns.loc[idxs].ln \n",
    "    patterns.loc[idxs, \"pdist\"] = pattern_lengths - 500\n",
    "    \n",
    "def process_raw_data(path, stem, rt):\n",
    "    df = load_raw_data(path, stem)\n",
    "    df.drop(df[df.rt != rt].index, inplace=True)\n",
    "    patterns = load_patterns(rt)\n",
    "    shift_terminal_stop_pdists(patterns)\n",
    "    clean(df, patterns)\n",
    "    build_timetable(df, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived Data<a name=\"#derived\"></a>\n",
    "### Trip and Wait Times<a name=\"#trips_waits\"></a>\n",
    "\n",
    "`get_destination_stops`  \n",
    "Returns a list of the possible destination stops for a bus route, given the route's patterns DataFrame, the starting stop, and the travel direction.\n",
    "\n",
    "`timedelta_to_decimal`  \n",
    "Converts a a pandas `TimeDelta` to its duration in minutes rounded to two decimal places.\n",
    "\n",
    "`calculate_travel_times`  \n",
    "Calculates the travel times between an origin and destination stop given a timetable DataFrame.\n",
    "\n",
    "`calculate_wait_times`  \n",
    "Calculates the wait times between consecutive buses given a timetable DataFrame. If consecutive buses have different service days, sets the calculated wait time to `None`.\n",
    "\n",
    "`build_travel_waits_df`  \n",
    "Given a bus route's timetable DataFrame, pattern DataFrame, and travel direction, calculates the travel times between each bus stop on the route and the wait times between adjacent buses at each stop. Stores the calculations in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_destination_stops(patterns, stop, direction):\n",
    "    filtered = patterns[(patterns.stpnm == stop) & (patterns.rtdir == direction)]\n",
    "    query_str = \" | \".join([\"(pid == '{}' & seq > {})\".format(pid, seq) for pid, seq in zip(filtered.pid, filtered.seq)])\n",
    "    return list(patterns.query(query_str).stpnm.unique())\n",
    "\n",
    "def timedelta_to_decimal(td):\n",
    "    return round(abs((td / np.timedelta64(1, 'D')) * 1440), 2)\n",
    "\n",
    "def calculate_travel_times(df, origin, destinations):\n",
    "    df[destinations] = df[destinations].sub(df[origin], axis=0)\n",
    "    df[destinations] = df[destinations].apply(timedelta_to_decimal)\n",
    "    return df\n",
    "\n",
    "def calculate_wait_times(df, stop):\n",
    "    df[\"wait_time\"] = df[stop] - df[stop].shift(-1)\n",
    "        \n",
    "    # removes wait times calculated between service days\n",
    "    df[\"day_diff\"] = df.start_date - df.start_date.shift(-1)\n",
    "    df.loc[df.day_diff.dt.days != 0, \"wait_time\"] = None\n",
    "    \n",
    "    df.drop(columns=[stop, \"day_diff\"], inplace=True)\n",
    "    df.wait_time = df.wait_time.apply(timedelta_to_decimal)\n",
    "    return df\n",
    "\n",
    "def build_travel_waits_df(df, patterns, direction):\n",
    "    info_columns = [\"start_date\", \"pid\", \"tatripid\", \"rtdir\", \"day_of_week\", \"holiday\", \"decimal_time\", \"wait_time\"]\n",
    "    \n",
    "    stop_list = patterns[patterns.rtdir == direction].stpnm.dropna().unique()\n",
    "    directional_df = df.loc[df.rtdir == direction]\n",
    "    directional_df[stop_list] = directional_df[stop_list].apply(pd.to_datetime)\n",
    "    \n",
    "    travels_waits = []\n",
    "    for origin in stop_list:\n",
    "        destinations = get_destination_stops(patterns, origin, direction)\n",
    "        \n",
    "        if not destinations:\n",
    "            continue\n",
    "\n",
    "        orgin_and_dests = [origin] + destinations\n",
    "        sorted_df = directional_df.sort_values(by=origin) # sorting by arrival times in origin column\n",
    "        sorted_df[\"decimal_time\"] = sorted_df[origin].map(lambda x: round(x.hour + x.minute / 60.0, 2))\n",
    "\n",
    "        sorted_df = calculate_travel_times(sorted_df, origin, destinations)        \n",
    "        sorted_df = calculate_wait_times(sorted_df, origin)\n",
    "\n",
    "        melted_df = pd.melt(sorted_df, id_vars=info_columns, value_vars=destinations, var_name=\"destination\", value_name=\"travel_time\")\n",
    "        melted_df.dropna(subset=[\"wait_time\", \"travel_time\"], inplace=True)\n",
    "        melted_df[\"origin\"] = origin\n",
    "        travels_waits.append(melted_df)\n",
    "    return pd.concat(travels_waits, ignore_index=True)\n",
    "\n",
    "def write_travel_waits():\n",
    "    #travel_waits_path = \"../../data/processed/trips_and_waits/\" + str(rt) + \"/\"\n",
    "    #check_if_path_exists(travel_waits_path)\n",
    "    #file_name = travel_waits_path + origin.replace(\"/\", \"\").replace(\".\", \"\") + \"_\" + direction + \".csv\"\n",
    "    #melted_df.to_csv(file_name, columns=header, header=False, index=False, mode='ab+')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
